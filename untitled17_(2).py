# -*- coding: utf-8 -*-
"""Untitled17 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RWJrU5VlZKe0YIPqnMesOn1hz9M8Vrr4
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt





df= pd.read_csv("/content/Banglore.csv")

df.describe()

import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib.pyplot as plt

df.columns



import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional, GRU, Conv1D, SimpleRNN
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn.metrics import mean_absolute_error, r2_score

# Define the functions for evaluation metrics
def calculate_rmse(y_true, y_pred):
    return sqrt(mean_squared_error(y_true, y_pred))

def calculate_mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def calculate_mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

import numpy as np

def calculate_mape(y_true, y_pred):
    # Calculate absolute errors
    abs_errors = np.abs(y_true - y_pred)

    # Calculate percentage errors, handling division by zero
    percentage_errors = np.divide(abs_errors, y_true, out=np.zeros_like(abs_errors), where=y_true!=0)

    # Calculate mean percentage error
    mape = np.mean(percentage_errors) * 100

    return mape

def calculate_r2(y_true, y_pred):
    ssr = np.sum((y_true - y_pred) ** 2)
    sst = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 1 - (ssr / sst)
    return r2

if 'TS' in df.columns:
    # Extract the time series data
    ts_data = df['TS'].values
else:
    print("Column 'TS' not found in DataFrame 'df'")

df.info()

df.columns

df.columns = ['P','TS']

# Assuming you have 'df' with a datetime column 'TS'

# Extract the time series data
ts_data = df['TS'].values



# Print the DataFrame columns
print(df.columns)

# Check if the column name is misspelled
if 'TS' not in df.columns:
    print("The column name 'TS' is not found in the DataFrame.")

# Normalize the data
scaler = MinMaxScaler()
ts_data_scaled = scaler.fit_transform(ts_data.reshape(-1, 1))

# Define the number of time steps for the LSTM model
sequence_length = 10  # You can adjust this as needed

# Create sequences for training, validation, and test sets
X, y = [], []
for i in range(len(ts_data_scaled) - sequence_length):
    X.append(ts_data_scaled[i:i + sequence_length])
    y.append(ts_data_scaled[i + sequence_length])

X, y = np.array(X), np.array(y)
train_split = int(0.6 * len(X))
val_split = int(0.2 * len(X)) + train_split
X_train, y_train = X[:train_split], y[:train_split]
X_val, y_val = X[train_split:val_split], y[train_split:val_split]
X_test, y_test = X[val_split:], y[val_split:]

# Function to create and train a BiLSTM model
def create_bilstm_model(input_shape):
    model = Sequential()
    model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# Function to create and train a GRU model
def create_gru_model(input_shape):
    model = Sequential()
    model.add(GRU(50, activation='relu', input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# Function to create and train a CNN model
def create_cnn_model(input_shape):
    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# Function to create and train an RNN model
def create_rnn_model(input_shape):
    model = Sequential()
    model.add(SimpleRNN(50, activation='relu', input_shape=input_shape))
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import MinMaxScaler
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.layers import Input, Conv1D, LSTM, GRU, Dense, Attention, Concatenate, TimeDistributed

# # Function to create and train the hybrid model
# def create_hybrid_model(input_shape):
#     input_layer = Input(shape=input_shape)

#     # CNN layers for initial feature extraction
#     cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)
#     cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(cnn_layer)

#     # DRNN layers for temporal feature extraction
#     drnn_layer = LSTM(64, return_sequences=True)(cnn_layer)
#     drnn_layer = GRU(64, return_sequences=True)(drnn_layer)
#     drnn_layer = LSTM(64, return_sequences=True)(drnn_layer)
#     drnn_layer = GRU(64, return_sequences=True)(drnn_layer)

#     # Attention mechanism
#     attention_layer = Attention()([drnn_layer, drnn_layer])

#     # Concatenate attention output with DRNN output
#     concatenated = Concatenate()([drnn_layer, attention_layer])

#     # TimeDistributed layer to output predictions for each time step
#     output_layer = TimeDistributed(Dense(1, activation='linear'))(concatenated)

#     # Build and compile the model
#     model = Model(inputs=input_layer, outputs=output_layer)
#     model.compile(optimizer='adam', loss='mse')

#     return model

# import tensorflow as tf
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate, TimeDistributed

# def create_hybrid_rnn_model(input_shape):
#     # Define the input layer
#     input_layer = Input(shape=input_shape)

#     # Add LSTM layers for temporal feature extraction
#     lstm_layer = LSTM(64, return_sequences=True)(input_layer)
#     lstm_layer = LSTM(64, return_sequences=True)(lstm_layer)

#     # Attention mechanism
#     attention_layer = Attention()([lstm_layer, lstm_layer])

#     # Concatenate attention output with LSTM output
#     concatenated = Concatenate()([lstm_layer, attention_layer])

#     # TimeDistributed layer to output predictions for each time step
#     output_layer = TimeDistributed(Dense(1, activation='linear'))(concatenated)

#     # Build and compile the model
#     model = Model(inputs=input_layer, outputs=output_layer)
#     model.compile(optimizer='adam', loss='mse')

#     return model

# # Define timesteps and features
# timesteps = 10  # Replace with the number of timesteps in your input data
# features = 5    # Replace with the number of features in your input data

# # Example usage
# input_shape = (timesteps, features)
# model = create_hybrid_rnn_model(input_shape)
# model.summary()

# import tensorflow as tf
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Concatenate, TimeDistributed

# def create_hybrid_rnn_model(input_shape):
#     # Define the input layer
#     input_layer = Input(shape=input_shape)

#     # Add LSTM layers for temporal feature extraction
#     lstm_layer = LSTM(64, return_sequences=True)(input_layer)
#     lstm_layer = LSTM(64, return_sequences=True)(lstm_layer)

#     # Attention mechanism
#     attention_layer = Attention()([lstm_layer, lstm_layer])

#     # Concatenate attention output with LSTM output
#     concatenated = Concatenate()([lstm_layer, attention_layer])

#     # TimeDistributed layer to output predictions for each time step
#     output_layer = TimeDistributed(Dense(1, activation='linear'))(concatenated)

#     # Build and compile the model
#     model = Model(inputs=input_layer, outputs=output_layer)
#     model.compile(optimizer='adam', loss='mse')

#     return model

# # Example usage
# input_shape = (timesteps, features)  # Replace with your actual input shape
# model = create_hybrid_rnn_model(input_shape)
# model.summary()

# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import SimpleRNN, Dense

# # Define the input shape
# input_shape = (10, 1)  # Example input shape: 10 time steps, 1 feature

# def create_stacked_rnn(input_shape):
#     model = Sequential()
#     model.add(SimpleRNN(50, activation='relu', return_sequences=True, input_shape=input_shape))
#     model.add(SimpleRNN(50, activation='relu', return_sequences=True))
#     model.add(SimpleRNN(50, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# # Create the model
# model = create_stacked_rnn(input_shape)
# model.summary()

# from tensorflow.keras.layers import Dropout

# def create_lstm_with_dropout(input_shape):
#     model = Sequential()
#     model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))
#     model.add(Dropout(0.2))
#     model.add(LSTM(50, activation='relu', return_sequences=True))
#     model.add(Dropout(0.2))
#     model.add(LSTM(50, activation='relu'))
#     model.add(Dropout(0.2))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# # Create the model
# model_lstm_dropout = create_lstm_with_dropout(input_shape)
# model_lstm_dropout.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, LSTM, Dense

def create_stacked_rnn_lstm_hybrid(input_shape):
    model = Sequential()
    # First RNN layer
    model.add(SimpleRNN(50, activation='relu', return_sequences=True, input_shape=input_shape))
    # First LSTM layer
    model.add(LSTM(50, activation='relu', return_sequences=True))
    # Second RNN layer
    model.add(SimpleRNN(50, activation='relu', return_sequences=True))
    # Second LSTM layer
    model.add(LSTM(50, activation='relu'))
    # Output layer
    model.add(Dense(1))
    model.compile(optimizer='adam', loss='mse')
    return model

# Define the input shape
input_shape = (10, 1)  # Example input shape: 10 time steps, 1 feature

# Create the model
model_hybrid = create_stacked_rnn_lstm_hybrid(input_shape)
model_hybrid.summary()

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, Dense

# def create_stacked_lstm():
#     input_shape = (10, 1)  # Example input shape: 10 time steps, 1 feature
#     model = Sequential()
#     model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))
#     model.add(LSTM(50, activation='relu', return_sequences=True))
#     model.add(LSTM(50, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# # Create the model
# model_lstm = create_stacked_lstm()
# model_lstm.summary()

# from tensorflow.keras.layers import Add, Input, RNN
# from tensorflow.keras.models import Model

# def residual_rnn_cell(x, units):
#     rnn_out = SimpleRNN(units, activation='relu', return_sequences=True)(x)
#     return Add()([x, rnn_out])

# def create_drnn_with_shortcut(input_shape):
#     inputs = Input(shape=input_shape)
#     x = SimpleRNN(50, activation='relu', return_sequences=True)(inputs)
#     x = residual_rnn_cell(x, 50)
#     x = SimpleRNN(50, activation='relu')(x)
#     outputs = Dense(1)(x)
#     model = Model(inputs, outputs)
#     model.compile(optimizer='adam', loss='mse')
#     return model

# model = create_drnn_with_shortcut(input_shape)
# model.summary()

# def create_drnn_deep_transition_output(input_shape):
#     model = Sequential()
#     model.add(SimpleRNN(50, activation='relu', return_sequences=True, input_shape=input_shape))
#     model.add(SimpleRNN(50, activation='relu'))
#     model.add(Dense(50, activation='relu'))
#     model.add(Dense(50, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# model = create_drnn_deep_transition_output(input_shape)
# model.summary()

# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import SimpleRNN, Dense, Attention, Input
# from tensorflow.keras.models import Model

# def create_rnn_attention_model(input_shape):
#     inputs = Input(shape=input_shape)
#     # RNN layer
#     rnn_out = SimpleRNN(50, activation='relu', return_sequences=True)(inputs)
#     # Attention mechanism
#     attention = Attention()([rnn_out, rnn_out])
#     # Flatten and Dense layer
#     flatten = tf.keras.layers.Flatten()(attention)
#     outputs = Dense(1)(flatten)
#     # Compile model
#     model = Model(inputs=inputs, outputs=outputs)
#     model.compile(optimizer='adam', loss='mse')
#     return model

# # Example usage
# input_shape = (10, 1)  # Example input shape: 10 time steps, 1 feature
# model = create_rnn_attention_model(input_shape)
# model.summary()

# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout
# from tensorflow.keras.regularizers import l2

# def create_optimized_drnn_model(input_shape):
#     model = Sequential()

#     # Add multiple RNN layers with dropout and regularization
#     model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=input_shape, kernel_regularizer=l2(0.001)))
#     model.add(Dropout(0.2))
#     model.add(LSTM(100, activation='relu', return_sequences=True, kernel_regularizer=l2(0.001)))
#     model.add(Dropout(0.2))
#     model.add(LSTM(100, activation='relu', kernel_regularizer=l2(0.001)))
#     model.add(Dropout(0.2))

#     # Add a dense output layer
#     model.add(Dense(1))

#     # Compile the model with a lower learning rate
#     optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
#     model.compile(optimizer=optimizer, loss='mse')

#     return model

# # Example usage
# input_shape = (10, 1)  # Example input shape: 10 time steps, 1 feature
# model = create_optimized_drnn_model(input_shape)
# model.summary()

# # Generate some dummy sequential data
# import numpy as np

# X_train = np.random.rand(1000, 10, 1)  # 1000 samples, 10 time steps, 1 feature
# y_train = np.random.rand(1000, 1)  # 1000 target values

# # Train the model
# history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import SimpleRNN, Dense

# def create_drnn_model(input_shape):
#     model = Sequential()
#     # Add multiple RNN layers
#     model.add(SimpleRNN(50, activation='relu', return_sequences=True, input_shape=input_shape))
#     model.add(SimpleRNN(50, activation='relu', return_sequences=True))
#     model.add(SimpleRNN(50, activation='relu'))
#     # Add a dense output layer
#     model.add(Dense(1))
#     # Compile the model
#     model.compile(optimizer='adam', loss='mse')
#     return model

# # Example usage
# input_shape = (10, 1)  # Example input shape: 10 time steps, 1 feature
# model = create_drnn_model(input_shape)
# model.summary()

# from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention, Dropout, Embedding
# from tensorflow.keras.models import Model

# def create_transformer_model(input_shape, num_layers, d_model, num_heads):
#     inputs = Input(shape=input_shape)
#     x = inputs

#     for _ in range(num_layers):
#         attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
#         attention_output = Dropout(0.1)(attention_output)
#         x = LayerNormalization(epsilon=1e-6)(x + attention_output)

#         ffn_output = Dense(d_model, activation='relu')(x)
#         ffn_output = Dense(input_shape[-1])(ffn_output)
#         ffn_output = Dropout(0.1)(ffn_output)
#         x = LayerNormalization(epsilon=1e-6)(x + ffn_output)

#     x = Dense(1)(x)  # Output layer
#     model = Model(inputs=inputs, outputs=x)
#     model.compile(optimizer='adam', loss='mse')
#     return model

# import torch
# import torch.nn as nn
# import torch.optim as optim
# from torch.utils.data import DataLoader, TensorDataset

# # Define the LSTM model
# class LSTMModel(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(LSTMModel, self).__init__()
#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
#         self.linear = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         lstm_out, _ = self.lstm(x)
#         lstm_out = lstm_out[:, -1, :]  # take the output of the last time step
#         output = self.linear(lstm_out)
#         return output

# # Hyperparameters
# input_size = 10  # Number of features
# hidden_size = 50
# output_size = 1
# num_epochs = 20
# learning_rate = 0.001

# # Create the model, define the loss function and the optimizer
# model = LSTMModel(input_size, hidden_size, output_size)
# criterion = nn.MSELoss()
# optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# # Dummy dataset
# # Replace with actual data
# x_train = torch.randn(1000, 168, input_size)  # (num_samples, sequence_length, num_features)
# y_train = torch.randn(1000, output_size)
# train_dataset = TensorDataset(x_train, y_train)
# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# # Training loop
# for epoch in range(num_epochs):
#     for x_batch, y_batch in train_loader:
#         optimizer.zero_grad()
#         outputs = model(x_batch)
#         loss = criterion(outputs, y_batch)
#         loss.backward()
#         optimizer.step()
#     print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')

# # Save the model
# torch.save(model.state_dict(), 'lstm_model.pth')

# import torch
# from torch import nn, optim
# from torch.optim.optimizer import Optimizer
# from torch.utils.data import DataLoader, TensorDataset
# import math

# # Define the RAdam optimizer
# class RAdam(Optimizer):
#     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):
#         if not 0.0 <= lr:
#             raise ValueError("Invalid learning rate: {}".format(lr))
#         if not 0.0 <= eps:
#             raise ValueError("Invalid epsilon value: {}".format(eps))
#         if not 0.0 <= betas[0] < 1.0:
#             raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
#         if not 0.0 <= betas[1] < 1.0:
#             raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

#         self.degenerated_to_sgd = degenerated_to_sgd
#         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])
#         super(RAdam, self).__init__(params, defaults)

#     def __setstate__(self, state):
#         super(RAdam, self).__setstate__(state)

#     def step(self, closure=None):
#         loss = None
#         if closure is not None:
#             loss = closure()

#         for group in self.param_groups:
#             for p in group['params']:
#                 if p.grad is None:
#                     continue
#                 grad = p.grad.data.float()
#                 if grad.is_sparse:
#                     raise RuntimeError('RAdam does not support sparse gradients')

#                 p_data_fp32 = p.data.float()

#                 state = self.state[p]

#                 if len(state) == 0:
#                     state['step'] = 0
#                     state['exp_avg'] = torch.zeros_like(p_data_fp32)
#                     state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)
#                 else:
#                     state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)
#                     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)

#                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
#                 beta1, beta2 = group['betas']

#                 exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
#                 exp_avg.mul_(beta1).add_(1 - beta1, grad)

#                 state['step'] += 1
#                 buffered = group['buffer'][int(state['step'] % 10)]
#                 if state['step'] == buffered[0]:
#                     N_sma, step_size = buffered[1], buffered[2]
#                 else:
#                     buffered[0] = state['step']
#                     beta2_t = beta2 ** state['step']
#                     N_sma_max = 2 / (1 - beta2) - 1
#                     N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)
#                     buffered[1] = N_sma

#                     if N_sma >= 5:
#                         step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])
#                     elif self.degenerated_to_sgd:
#                         step_size = 1.0 / (1 - beta1 ** state['step'])
#                     else:
#                         step_size = -1
#                     buffered[2] = step_size

#                 if N_sma >= 5:
#                     if group['weight_decay'] != 0:
#                         p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)
#                     denom = exp_avg_sq.sqrt().add_(group['eps'])
#                     p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)
#                     p.data.copy_(p_data_fp32)
#                 elif step_size > 0:
#                     if group['weight_decay'] != 0:
#                         p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)
#                     p_data_fp32.add_(-step_size * group['lr'], exp_avg)
#                     p.data.copy_(p_data_fp32)

#         return loss

# # Define the example model
# class ExampleModel(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(ExampleModel, self).__init__()
#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         h, _ = self.lstm(x)
#         h = h[:, -1, :]  # Take the last output of the sequence
#         out = self.fc(h)
#         return out

# # Example data
# input_size = 18
# hidden_size = 50
# output_size = 1
# inputs = torch.randn(1000, 256, input_size)
# targets = torch.randn(1000, output_size)

# # Normalize data
# inputs_mean = inputs.mean(dim=0, keepdim=True)
# inputs_std = inputs.std(dim=0, keepdim=True)
# inputs = (inputs - inputs_mean) / inputs_std

# # DataLoader
# dataset = TensorDataset(inputs, targets)
# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# # Initialize the model, loss function, optimizer, and scheduler
# device = 'cuda' if torch.cuda.is_available() else 'cpu'
# model = ExampleModel(input_size, hidden_size, output_size).to(device)
# criterion = nn.MSELoss()
# optimizer = RAdam(model.parameters(), lr=1e-3)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# # Training loop
# num_epochs = 100
# for epoch in range(num_epochs):
#     model.train()
#     for batch_inputs, batch_targets in dataloader:
#         batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)

#         optimizer.zero_grad()
#         outputs = model(batch_inputs)
#         loss = criterion(outputs, batch_targets)
#         loss.backward()

#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping

#         optimizer.step()

#     scheduler.step()  # Step the learning rate scheduler

#     print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# # Save trained model parameters if needed
# # torch.save(model.state_dict(), 'model.pth')

# import torch
# from torch import nn, optim
# from torch.optim.optimizer import Optimizer
# from torch.utils.data import DataLoader, TensorDataset
# import numpy as np
# import math

# # Define the RAdam optimizer
# class RAdam(Optimizer):
#     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):
#         # Initialization code here

#      def step(self, closure=None):  # Add indentation here
#         # Optimizer step code here  # Add indentation here
#         if closure is not None:
#             with torch.enable_grad():
#                 closure()

# # Define the example model
# class ExampleModel(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         # Model architecture definition

#      def forward(self, x):
#         # Forward pass definition

# # ... rest of the code remains the same ...

# # Example data
#            input_size = 18
#              hidden_size = 50
# output_size = 1
# inputs = torch.randn(1000, 256, 18)
# targets = torch.randn(1000, 1)

# # Normalize data
# inputs_mean = inputs.mean(dim=0, keepdim=True)
# inputs_std = inputs.std(dim=0, keepdim=True)
# inputs = (inputs - inputs_mean) / inputs_std

# # DataLoader
# dataset = TensorDataset(inputs, targets)
# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# # Initialize the model, loss function, optimizer, and scheduler
# model = ExampleModel(input_size, hidden_size, output_size).to('cuda' if torch.cuda.is_available() else 'cpu')
# criterion = nn.MSELoss()
# optimizer = RAdam(model.parameters(), lr=1e-3)
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# # Training loop
# num_epochs = 100
# for epoch in range(num_epochs):
#     model.train()
#     for batch_inputs, batch_targets in dataloader:
#         batch_inputs, batch_targets = batch_inputs.to('cuda' if torch.cuda.is_available() else 'cpu'), batch_targets.to('cuda' if torch.cuda.is_available() else 'cpu')

#         optimizer.zero_grad()
#         outputs = model(batch_inputs)
#         loss = criterion(outputs, batch_targets)
#         loss.backward()

#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping

#         optimizer.step()

#     scheduler.step()  # Step the learning rate scheduler

#     # Validation (optional): Evaluate model on validation set here if available

#     print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# # Save trained model parameters if needed
# # torch.save(model.state_dict(), 'model.pth')

# import math
# import torch
# from torch import nn, optim
# from torch.optim.optimizer import Optimizer
# from torch.utils.data import DataLoader, TensorDataset
# import numpy as np

# # Define the RAdam optimizer
# class RAdam(Optimizer):
#     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):
#         if not 0.0 <= lr:
#             raise ValueError("Invalid learning rate: {}".format(lr))
#         if not 0.0 <= eps:
#             raise ValueError("Invalid epsilon value: {}".format(eps))
#         if not 0.0 <= betas[0] < 1.0:
#             raise ValueError("Invalid beta parameter at index 0: {}".format(betas[0]))
#         if not 0.0 <= betas[1] < 1.0:
#             raise ValueError("Invalid beta parameter at index 1: {}".format(betas[1]))

#         self.degenerated_to_sgd = degenerated_to_sgd
#         defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])
#         super(RAdam, self).__init__(params, defaults)

#     def __setstate__(self, state):
#         super(RAdam, self).__setstate__(state)

#     def step(self, closure=None):
#         loss = None
#         if closure is not None:
#             loss = closure()

#         for group in self.param_groups:
#             for p in group['params']:
#                 if p.grad is None:
#                     continue
#                 grad = p.grad.data.float()
#                 if grad.is_sparse:
#                     raise RuntimeError('RAdam does not support sparse gradients')

#                 p_data_fp32 = p.data.float()

#                 state = self.state[p]

#                 if len(state) == 0:
#                     state['step'] = 0
#                     state['exp_avg'] = torch.zeros_like(p_data_fp32)
#                     state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)
#                 else:
#                     state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)
#                     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)

#                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
#                 beta1, beta2 = group['betas']

#                 exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
#                 exp_avg.mul_(beta1).add_(1 - beta1, grad)

#                 state['step'] += 1
#                 buffered = group['buffer'][int(state['step'] % 10)]
#                 if state['step'] == buffered[0]:
#                     N_sma, step_size = buffered[1], buffered[2]
#                 else:
#                     buffered[0] = state['step']
#                     beta2_t = beta2 ** state['step']
#                     N_sma_max = 2 / (1 - beta2) - 1
#                     N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)
#                     buffered[1] = N_sma

#                     if N_sma >= 5:
#                         step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])
#                     elif self.degenerated_to_sgd:
#                         step_size = 1.0 / (1 - beta1 ** state['step'])
#                     else:
#                         step_size = -1
#                     buffered[2] = step_size

#                 if N_sma >= 5:
#                     if group['weight_decay'] != 0:
#                         p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)
#                     denom = exp_avg_sq.sqrt().add_(group['eps'])
#                     p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)
#                     p.data.copy_(p_data_fp32)
#                 elif step_size > 0:
#                     if group['weight_decay'] != 0:
#                         p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)
#                     p_data_fp32.add_(-step_size * group['lr'], exp_avg)
#                     p.data.copy_(p_data_fp32)

#         return loss

# # Define the example model
# class ExampleModel(nn.Module):
#     def __init__(self, input_size, hidden_size, output_size):
#         super(ExampleModel, self).__init__()
#         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
#         self.fc = nn.Linear(hidden_size, output_size)

#     def forward(self, x):
#         h, _ = self.lstm(x)
#         h = h[:, -1, :]  # Take the last output of the sequence
#         out = self.fc(h)
#         return out

# # Initialize the model, loss function, and optimizer
# input_size = 18
# hidden_size = 50
# output_size = 1
# model = ExampleModel(input_size, hidden_size, output_size).to('cuda' if torch.cuda.is_available() else 'cpu')
# criterion = nn.MSELoss()
# optimizer = RAdam(model.parameters(), lr=1e-3)

# # Learning rate scheduler
# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

# # Example data
# inputs = torch.randn(1000, 256, 18)  # Example input (batch_size, seq_len, input_size)
# targets = torch.randn(1000, 1)       # Example targets

# # Normalize data
# inputs_mean = inputs.mean(dim=0, keepdim=True)
# inputs_std = inputs.std(dim=0, keepdim=True)
# inputs = (inputs - inputs_mean) / inputs_std

# # DataLoader
# dataset = TensorDataset(inputs, targets)
# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# # Example training loop
# for epoch in range(100):
#     model.train()
#     for batch_inputs, batch_targets in dataloader:
#         batch_inputs, batch_targets = batch_inputs.to('cuda' if torch.cuda.is_available() else 'cpu'), batch_targets.to('cuda' if torch.cuda.is_available() else 'cpu')
#         optimizer.zero_grad()
#         outputs = model(batch_inputs)
#         loss = criterion(outputs, batch_targets)
#         loss.backward()

#         # Gradient clipping
#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

#         optimizer.step()

#     scheduler.step()  # Step the learning rate scheduler

#     print(f"Epoch {epoch+1}, Loss: {loss.item()}")





# import tensorflow as tf
# from tensorflow.keras.layers import Dense, BatchNormalization, Input, LayerNormalization, MultiHeadAttention, Add, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# def transformer_encoder(inputs, d_model, n_head, dim_feedforward, dropout_rate=0.1):
#     # Multi-head attention
#     attn_output = MultiHeadAttention(num_heads=n_head, key_dim=d_model)(inputs, inputs)
#     attn_output = Add()([inputs, attn_output])
#     attn_output = LayerNormalization(epsilon=1e-6)(attn_output)
#     attn_output = Dropout(dropout_rate)(attn_output)

#     # Feedforward network
#     ffn_output = Dense(dim_feedforward, activation='relu')(attn_output)
#     ffn_output = Dense(d_model)(ffn_output)
#     ffn_output = Add()([attn_output, ffn_output])
#     ffn_output = LayerNormalization(epsilon=1e-6)(ffn_output)
#     ffn_output = Dropout(dropout_rate)(ffn_output)

#     return ffn_output

# # Function to create and train the Transformer model
# def create_transformer_model(input_shape, seq_len=256, ini_len=18, final_len=1):
#     d_model = 64  # Increased model dimension
#     n_head = 8    # Increased number of heads
#     dim_feedforward = 2048
#     dropout_rate = 0.1  # Dropout rate

#     inputs = Input(shape=input_shape)

#     # Initial transformation
#     x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)
#     x = Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
#     x = Dense(d_model, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)

#     # Batch normalization
#     x = BatchNormalization()(x)

#     # Transformer Encoders
#     for _ in range(4):  # Increased the number of transformer layers
#         x = transformer_encoder(x, d_model, n_head, dim_feedforward, dropout_rate)

#     # Flattening and final Dense layers
#     x = tf.reshape(x, [-1, d_model * seq_len])
#     x = Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)
#     x = Dropout(dropout_rate)(x)
#     outputs = Dense(final_len)(x)

#     model = Model(inputs, outputs)
#     model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')  # Adjusted learning rate

#     return model

# # Example usage
# input_shape = (256, 18)  # seq_len=256, ini_len=18
# model = create_transformer_model(input_shape)
# model.summary()

# # Callbacks
# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

# # Assuming you have your data loaded in variables X_train, y_train, X_val, y_val
# # model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32, callbacks=[early_stopping, checkpoint])

# import tensorflow as tf
# from tensorflow.keras.layers import Dense, BatchNormalization, Input, LayerNormalization, MultiHeadAttention, Add
# from tensorflow.keras.models import Model
# from tensorflow.keras.optimizers import Adam

# def transformer_encoder(inputs, d_model, n_head, dim_feedforward):
#     # Multi-head attention
#     attn_output = MultiHeadAttention(num_heads=n_head, key_dim=d_model)(inputs, inputs)
#     attn_output = Add()([inputs, attn_output])
#     out1 = LayerNormalization(epsilon=1e-6)(attn_output)

#     # Feedforward network
#     ffn_output = Dense(dim_feedforward, activation='relu')(out1)
#     ffn_output = Dense(d_model)(ffn_output)
#     ffn_output = Add()([out1, ffn_output])
#     return LayerNormalization(epsilon=1e-6)(ffn_output)

# # Function to create and train the Transformer model
# def create_transformer_model(input_shape, seq_len=256, ini_len=18, final_len=1):
#     d_model = 20
#     n_head = 4
#     dim_feedforward = 2048

#     inputs = Input(shape=input_shape)

#     # Initial transformation
#     x = Dense(32, activation='relu')(inputs)
#     x = Dense(32, activation='relu')(x)
#     x = Dense(d_model, activation='relu')(x)

#     # Batch normalization
#     x = BatchNormalization()(x)

#     # First Transformer Encoder
#     x = transformer_encoder(x, d_model, n_head, dim_feedforward)

#     # Second Transformer Encoder
#     x = transformer_encoder(x, d_model, n_head, dim_feedforward)

#     # Flattening and final Dense layers
#     x = tf.reshape(x, [-1, d_model * seq_len])
#     x = Dense(512, activation='relu')(x)
#     outputs = Dense(final_len)(x)

#     model = Model(inputs, outputs)
#     model.compile(optimizer=Adam(), loss='mse')

#     return model

# # Example usage
# input_shape = (256, 18)  # seq_len=256, ini_len=18
# model = create_transformer_model(input_shape)
# model.summary()

# import numpy as np
# from keras.models import Sequential
# from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, LSTM, BatchNormalization, Input

# # Function to create and train the CNN-LSTM model
# def create_cnn_lstm_model(input_shape):
#     model = Sequential()

#     # Initial transformation
#     model.add(Input(shape=input_shape))
#     model.add(Dense(32, activation='relu'))
#     model.add(Dense(32, activation='relu'))
#     model.add(Dense(20, activation='relu'))

#     # Batch normalization
#     model.add(BatchNormalization())

#     # Convolutional layers
#     model.add(Conv1D(32, 4, activation='relu', padding='same'))
#     model.add(Conv1D(16, 4, activation='relu', padding='same'))
#     model.add(Conv1D(16, 4, activation='relu', padding='same'))
#     model.add(MaxPooling1D(2))
#     model.add(Conv1D(20, 4, activation='relu', padding='same'))

#     # LSTM layer
#     model.add(LSTM(32, return_sequences=True))

#     # Flattening the output and final Dense layers
#     model.add(Flatten())
#     model.add(Dense(512, activation='relu'))
#     model.add(Dense(1))

#     # Compile the model
#     model.compile(optimizer=Adam(), loss='mse')

#     return model

# # Example usage
# input_shape = (128, 18)  # seq_len=128, ini_len=18
# model = create_cnn_lstm_model(input_shape)
# model.summary()



# #hybrid of cnn lstm
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten, Reshape

# def create_hybrid_model(input_shape):
#     model = Sequential()
#     # CNN layers
#     model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape))
#     model.add(MaxPooling1D(pool_size=2))
#     model.add(Conv1D(128, kernel_size=3, activation='relu'))
#     model.add(MaxPooling1D(pool_size=2))
#     model.add(Flatten())
#     model.add(Reshape((-1, 128)))  # Reshape output for LSTM input
#     # LSTM layers
#     model.add(LSTM(50, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import ConvLSTM2D, Dense

# def create_conv_lstm_model(input_shape):
#     model = Sequential()
#     model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model
 #ConvLSTM model

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, SimpleRNN, Dense

# def create_hybrid_model(input_shape):
#     model = Sequential()
#     model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))  # LSTM layer
#     model.add(SimpleRNN(50, activation='relu'))  # RNN layer
#     model.add(Dense(1))  # Output layer
#     model.compile(optimizer='adam', loss='mse')
#     return model
# #lstm rnn hybrid

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import LSTM, SimpleRNN, Dense

# def create_hybrid_model(input_shape):
#     model = Sequential()
#     model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))  # LSTM layer
#     model.add(SimpleRNN(50, activation='relu'))  # RNN layer
#     model.add(Dense(1))  # Output layer
#     model.compile(optimizer='adam', loss='mse')
#     return model
#rnn lstm hybrid

# def create_hybrid_model(input_shape):
#     model = Sequential()
#     model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=input_shape))
#     model.add(GRU(50, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# import numpy as np
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Dense
# from tensorflow.keras.optimizers import Adam

# # Function to create the CNN-LSTM hybrid model
# def create_cnn_lstm_model(input_shape):
#     model = Sequential()

#     # CNN part
#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))
#     model.add(MaxPooling1D(pool_size=2))
#     model.add(Flatten())

#     # LSTM part
#     model.add(LSTM(50, activation='relu'))

#     # Output layer
#     model.add(Dense(1))

#     # Compile the model
#     model.compile(optimizer=Adam(), loss='mse')

#     return model

# # Function to train the model
# def train_model(model, X_train, y_train, epochs=10, batch_size=32):
#     history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)
#     return history

# # Example usage
# if __name__ == "__main__":
#     # Generate dummy data
#     X_train = np.random.rand(1000, 10, 1)  # 1000 samples, 10 timesteps, 1 feature
#     y_train = np.random.rand(1000)         # 1000 target values

#     # Create the model
#     input_shape = (X_train.shape[1], X_train.shape[2])
#     model = create_cnn_lstm_model(input_shape)

#     # Train the model
#     history = train_model(model, X_train, y_train, epochs=10, batch_size=32)

#     # Print the model summary
#     model.summary()

# import numpy as np
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense
# from tensorflow.keras.optimizers import Adam

# # Function to create the CNN-LSTM hybrid model
# def create_cnn_lstm_model(input_shape):
#     model = Sequential()

#     # CNN part
#     model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))
#     model.add(MaxPooling1D(pool_size=2))

#     # LSTM part
#     model.add(LSTM(50, activation='relu'))

#     # Output layer
#     model.add(Dense(1))

#     # Compile the model
#     model.compile(optimizer=Adam(), loss='mse')

#     return model

# # Function to train the model
# def train_model(model, X_train, y_train, epochs=10, batch_size=32):
#     history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)
#     return history

# # Example usage
# if __name__ == "__main__":
#     # Generate dummy data
#     X_train = np.random.rand(1000, 10, 1)  # 1000 samples, 10 timesteps, 1 feature
#     y_train = np.random.rand(1000)         # 1000 target values

#     # Create the model
#     input_shape = (X_train.shape[1], X_train.shape[2])
#     model = create_cnn_lstm_model(input_shape)

#     # Train the model
#     history = train_model(model, X_train, y_train, epochs=10, batch_size=32)

#     # Print the model summary
#     model.summary()

# import numpy as np
# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Permute, Multiply, Lambda, concatenate
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras import backend as K

# # Custom Attention Layer
# def attention_block(inputs):
#     # Compute attention scores
#     a = Dense(inputs.shape[-1], activation='softmax')(inputs)
#     # Compute the context vector
#     output_attention = Multiply()([inputs, a])
#     return output_attention

# # Function to create the CNN-LSTM hybrid model with attention
# def create_cnn_lstm_attention_model(input_shape):
#     inputs = Input(shape=input_shape)

#     # CNN part
#     cnn_out = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)
#     cnn_out = MaxPooling1D(pool_size=2)(cnn_out)

#     # LSTM part
#     lstm_out = LSTM(50, return_sequences=True, activation='relu')(cnn_out)

#     # Attention mechanism
#     attention_out = attention_block(lstm_out)

#     # Combine context vector and LSTM output
#     merged_out = concatenate([attention_out, lstm_out])

#     # Output layer
#     output = Dense(1)(merged_out)

#     # Create the model
#     model = Model(inputs=inputs, outputs=output)

#     # Compile the model
#     model.compile(optimizer=Adam(), loss='mse')

#     return model

# # Function to train the model
# def train_model(model, X_train, y_train, epochs=10, batch_size=32):
#     history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)
#     return history

# # Example usage
# if __name__ == "__main__":
#     # Generate dummy data
#     X_train = np.random.rand(1000, 10, 1)  # 1000 samples, 10 timesteps, 1 feature
#     y_train = np.random.rand(1000)         # 1000 target values

#     # Create the model
#     input_shape = (X_train.shape[1], X_train.shape[2])
#     model = create_cnn_lstm_attention_model(input_shape)

#     # Train the model
#     history = train_model(model, X_train, y_train, epochs=10, batch_size=32)

#     # Print the model summary
#     model.summary()

# Function to train a model and make predictions
def train_and_predict_model(model_creator, X_train, y_train, X_test, scaler):
    model = model_creator(X_train.shape[1:])
    model.compile(optimizer='adam', loss='mse')  # Compile the model here
    model.fit(X_train, y_train, epochs=1, batch_size=32)

    # Reshape the test data to match the 3D input shape expected by LSTM
    X_test_3d = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    y_pred = model.predict(X_test_3d)
    y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1))
    return y_pred_inv

# def create_hybrid_model(input_shape):
#     model = Sequential()
#     model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=input_shape))
#     model.add(GRU(50, activation='relu'))
#     model.add(Dense(1))
#     model.compile(optimizer='adam', loss='mse')
#     return model

# List of models and their corresponding creators
models = [
    ("LSTM", create_bilstm_model),
    ("BiLSTM", create_bilstm_model),
    ("GRU", create_gru_model),
    ("CNN", create_cnn_model),
    ("RNN",create_rnn_model),
    ( "hybrid",create_stacked_rnn_lstm_hybrid),



]



# Results storage
results = {model[0]: {"RMSE": [], "MAE": [], "MSE": [], "MAPE": [], "R2": []} for model in models}

"""#***Matrices*** evaluation"""

# Additional list to store minimum values across 10 iterations
min_metrics = {"RMSE": float('inf'), "MSE": float('inf'), "MAE": float('inf'), "MAPE": float('inf'), "R2": -float('inf')}
min_iteration_metrics = {"RMSE": {"value": float('inf'), "iteration": 0, "model": ""},
                         "MSE": {"value": float('inf'), "iteration": 0, "model": ""},
                         "MAE": {"value": float('inf'), "iteration": 0, "model": ""},
                         "MAPE": {"value": float('inf'), "iteration": 0, "model": ""},
                         "R2": {"value": -float('inf'), "iteration": 0, "model": ""}}

# import numpy as np
# import pandas as pd

# # Define the function to calculate MAPE with handling for zero values
# def calculate_mape(y_true, y_pred, epsilon=1e-10):
#     return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

# # Number of iterations
# num_iterations = 15

# # Number of epochs
# num_epochs = 100

# # Initialize matrices to store evaluation metrics across iterations
# rmse_matrix = np.zeros((num_iterations, len(models)))
# mae_matrix = np.zeros((num_iterations, len(models)))
# mse_matrix = np.zeros((num_iterations, len(models)))
# r2_matrix = np.zeros((num_iterations, len(models)))
# mape_matrix = np.zeros((num_iterations, len(models)))

# All_model_dict = []
# y_pred_list = []

# # Initialize dictionary to track minimum metrics and corresponding iterations and models
# min_iteration_metrics = {
#     "RMSE": {"value": float('inf'), "iteration": None, "model": None},
#     "MSE": {"value": float('inf'), "iteration": None, "model": None},
#     "MAE": {"value": float('inf'), "iteration": None, "model": None},
#     "R2": {"value": float('-inf'), "iteration": None, "model": None},
#     "MAPE": {"value": float('inf'), "iteration": None, "model": None}
# }

# # Iteration loop
# for iteration in range(num_iterations):
#     print(f"\nIteration {iteration + 1}/{num_iterations}")

#     # Iterate over each model
#     for model_index, (model_name, model_creator) in enumerate(models):
#         # Train and predict
#         # Check shapes of X_train and y_train and ensure they are compatible
#         print("Shape of X_train:", X_train.shape)
#         print("Shape of y_train:", y_train.shape)
#         y_pred = train_and_predict_model(model_creator, X_train, y_train, X_test, scaler, epochs=num_epochs)

#         # Inverse transform the scaled true values
#         y_test_inv = scaler.inverse_transform(y_test)

#         # Ensure that the lengths match by trimming the longer prediction
#         min_length = min(len(y_pred), len(y_test_inv))
#         y_pred = y_pred[:min_length]
#         y_test_inv = y_test_inv[:min_length]

#         # Calculate metrics
#         rmse = calculate_rmse(y_test_inv, y_pred)
#         mse = calculate_mse(y_test_inv, y_pred)
#         mae = calculate_mae(y_test_inv, y_pred)
#         r2 = calculate_r2(y_test_inv, y_pred)
#         mape = calculate_mape(y_test_inv, y_pred)

#         # Store metrics in matrices
#         rmse_matrix[iteration, model_index] = rmse
#         mae_matrix[iteration, model_index] = mae
#         mse_matrix[iteration, model_index] = mse
#         r2_matrix[iteration, model_index] = r2
#         mape_matrix[iteration, model_index] = mape

#         # Update the minimum values and corresponding iteration
#         if rmse < min_iteration_metrics["RMSE"]["value"]:
#             min_iteration_metrics["RMSE"]["value"] = rmse
#             min_iteration_metrics["RMSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["RMSE"]["model"] = model_name
#         if mse < min_iteration_metrics["MSE"]["value"]:
#             min_iteration_metrics["MSE"]["value"] = mse
#             min_iteration_metrics["MSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MSE"]["model"] = model_name
#         if mae < min_iteration_metrics["MAE"]["value"]:
#             min_iteration_metrics["MAE"]["value"] = mae
#             min_iteration_metrics["MAE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAE"]["model"] = model_name
#         if r2 > min_iteration_metrics["R2"]["value"]:
#             min_iteration_metrics["R2"]["value"] = r2
#             min_iteration_metrics["R2"]["iteration"] = iteration + 1
#             min_iteration_metrics["R2"]["model"] = model_name
#         if mape < min_iteration_metrics["MAPE"]["value"]:
#             min_iteration_metrics["MAPE"]["value"] = mape
#             min_iteration_metrics["MAPE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAPE"]["model"] = model_name

#         # Print results for the current iteration and model
#         print(f"\n{model_name} - Iteration {iteration + 1}")
#         print(f"RMSE: {rmse:.2f}, MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}, MAPE: {mape:.2f}")

#         # Save actual and predicted values in an Excel sheet
#         df_results = pd.DataFrame({'Actual': np.squeeze(y_test_inv), 'Predicted': np.squeeze(y_pred)})

# # Calculate average metrics across all iterations
# average_rmse = np.mean(rmse_matrix, axis=0)
# average_mae = np.mean(mae_matrix, axis=0)
# average_mse = np.mean(mse_matrix, axis=0)
# average_r2 = np.mean(r2_matrix, axis=0)
# average_mape = np.mean(mape_matrix, axis=0)

# # Print average metrics for each model
# for model_index, (model_name, _) in enumerate(models):
#     print(f"\nAverage metrics for {model_name}:")
#     print(f"Average RMSE: {average_rmse[model_index]:.2f}")
#     print(f"Average MSE: {average_mse[model_index]:.2f}")
#     print(f"Average MAE: {average_mae[model_index]:.2f}")
#     print(f"Average R2: {average_r2[model_index]:.2f}")
#     print(f"Average MAPE: {average_mape[model_index]:.2f}")

import numpy as np
import pandas as pd

# # Define the function to calculate MAPE with handling for zero values
# def calculate_mape(y_true, y_pred, epsilon=1e-10):
#     return np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100

# Number of iterations
num_iterations = 1

# Initialize matrices to store evaluation metrics across iterations
rmse_matrix = np.zeros((num_iterations, len(models)))
mae_matrix = np.zeros((num_iterations, len(models)))
mse_matrix = np.zeros((num_iterations, len(models)))
r2_matrix = np.zeros((num_iterations, len(models)))
mape_matrix = np.zeros((num_iterations, len(models)))

All_model_dict = []
y_pred_list = []

# Initialize dictionary to track minimum metrics and corresponding iterations and models
min_iteration_metrics = {
    "RMSE": {"value": float('inf'), "iteration": None, "model": None},
    "MSE": {"value": float('inf'), "iteration": None, "model": None},
    "MAE": {"value": float('inf'), "iteration": None, "model": None},
    "R2": {"value": float('-inf'), "iteration": None, "model": None},
    "MAPE": {"value": float('inf'), "iteration": None, "model": None}
}

# Iteration loop
for iteration in range(num_iterations):
    print(f"\nIteration {iteration + 1}/{num_iterations}")

    # Iterate over each model
    for model_index, (model_name, model_creator) in enumerate(models):
        # Train and predict
        # Check shapes of X_train and y_train and ensure they are compatible
        print("Shape of X_train:", X_train.shape)
        print("Shape of y_train:", y_train.shape)
        y_pred = train_and_predict_model(model_creator, X_train, y_train, X_test, scaler)

        # Inverse transform the scaled true values
        y_test_inv = scaler.inverse_transform(y_test)

        # Ensure that the lengths match by trimming the longer prediction
        min_length = min(len(y_pred), len(y_test_inv))
        y_pred = y_pred[:min_length]
        y_test_inv = y_test_inv[:min_length]

        # Calculate metrics
        rmse = calculate_rmse(y_test_inv, y_pred)
        mse = calculate_mse(y_test_inv, y_pred)
        mae = calculate_mae(y_test_inv, y_pred)
        r2 = calculate_r2(y_test_inv, y_pred)
        mape = calculate_mape(y_test_inv, y_pred)

        # Store metrics in matrices
        rmse_matrix[iteration, model_index] = rmse
        mae_matrix[iteration, model_index] = mae
        mse_matrix[iteration, model_index] = mse
        r2_matrix[iteration, model_index] = r2
        mape_matrix[iteration, model_index] = mape

        # Update the minimum values and corresponding iteration
        if rmse < min_iteration_metrics["RMSE"]["value"]:
            min_iteration_metrics["RMSE"]["value"] = rmse
            min_iteration_metrics["RMSE"]["iteration"] = iteration + 1
            min_iteration_metrics["RMSE"]["model"] = model_name
        if mse < min_iteration_metrics["MSE"]["value"]:
            min_iteration_metrics["MSE"]["value"] = mse
            min_iteration_metrics["MSE"]["iteration"] = iteration + 1
            min_iteration_metrics["MSE"]["model"] = model_name
        if mae < min_iteration_metrics["MAE"]["value"]:
            min_iteration_metrics["MAE"]["value"] = mae
            min_iteration_metrics["MAE"]["iteration"] = iteration + 1
            min_iteration_metrics["MAE"]["model"] = model_name
        if r2 > min_iteration_metrics["R2"]["value"]:
            min_iteration_metrics["R2"]["value"] = r2
            min_iteration_metrics["R2"]["iteration"] = iteration + 1
            min_iteration_metrics["R2"]["model"] = model_name
        if mape < min_iteration_metrics["MAPE"]["value"]:
            min_iteration_metrics["MAPE"]["value"] = mape
            min_iteration_metrics["MAPE"]["iteration"] = iteration + 1
            min_iteration_metrics["MAPE"]["model"] = model_name

        # Print results for the current iteration and model
        print(f"\n{model_name} - Iteration {iteration + 1}")
        print(f"RMSE: {rmse:.2f}, MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}, MAPE: {mape:.2f}")

        # Save actual and predicted values in an Excel sheet
        df_results = pd.DataFrame({'Actual': np.squeeze(y_test_inv), 'Predicted': np.squeeze(y_pred)})

# Calculate average metrics across all iterations
average_rmse = np.mean(rmse_matrix, axis=0)
average_mae = np.mean(mae_matrix, axis=0)
average_mse = np.mean(mse_matrix, axis=0)
average_r2 = np.mean(r2_matrix, axis=0)
average_mape = np.mean(mape_matrix, axis=0)

# Print average metrics for each model
for model_index, (model_name, _) in enumerate(models):
    print(f"\nAverage metrics for {model_name}:")
    print(f"Average RMSE: {average_rmse[model_index]:.2f}")
    print(f"Average MSE: {average_mse[model_index]:.2f}")
    print(f"Average MAE: {average_mae[model_index]:.2f}")
    print(f"Average R2: {average_r2[model_index]:.2f}")
    print(f"Average MAPE: {average_mape[model_index]:.2f}")



import matplotlib.pyplot as plt
import seaborn as sns

# Assuming y_pred_list contains the best predictions for each model from the previous code
# and min_iteration_metrics contains the minimum metrics and corresponding model and iteration.

# Extract the best predictions for traditional and hybrid models
traditional_models = ["LSTM", "GRU", "CNN", "RNN", "BiLSTM"]
hybrid_models = ["Hybrid_LSTM_RNN"]

# Creating a dictionary for best predictions
best_test_predictions = {}
for model_name in traditional_models + hybrid_models:
    best_iteration = min_iteration_metrics["RMSE"]["iteration"] - 1  # Subtract 1 because iterations are 1-based in printout
    best_test_predictions[model_name] = y_pred_list[best_iteration]

# Define test_original_arr for actual values (assuming it's already defined)
# Using y_test_inv as the actual values from your previous code
actual_values = y_test_inv

# Create subplots for each comparison: traditional vs hybrid
fig, axes = plt.subplots(nrows=len(traditional_models), ncols=len(hybrid_models), figsize=(20, 25))
axes = axes.flatten()

for i, traditional_model in enumerate(traditional_models):
    for j, hybrid_model in enumerate(hybrid_models):
        idx = i * len(hybrid_models) + j
        sns.scatterplot(x=actual_values.flatten(), y=best_test_predictions[traditional_model].flatten(), ax=axes[idx], label=f'{traditional_model} Predictions', color='blue')
        sns.scatterplot(x=actual_values.flatten(), y=best_test_predictions[hybrid_model].flatten(), ax=axes[idx], label=f'{hybrid_model} Predictions', color='green')
        sns.lineplot(x=actual_values.flatten(), y=actual_values.flatten(), ax=axes[idx], color='red', label='Actual')
        axes[idx].set_title(f'{traditional_model} vs {hybrid_model}')
        axes[idx].set_xlabel('Actual Values')
        axes[idx].set_ylabel('Predicted Values')
        axes[idx].legend()

plt.tight_layout()
plt.show()

# import numpy as np
# import pandas as pd
# from sklearn.preprocessing import MinMaxScaler
# from tensorflow.keras.models import Model, Sequential
# from tensorflow.keras.layers import Input, Conv1D, LSTM, GRU, Dense, Attention, Concatenate, TimeDistributed

# # Function to create and train the hybrid model
# def create_hybrid_model(input_shape):
#     input_layer = Input(shape=input_shape)

#     # CNN layers for initial feature extraction
#     cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)
#     cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(cnn_layer)

#     # DRNN layers for temporal feature extraction
#     drnn_layer = LSTM(64, return_sequences=True)(cnn_layer)
#     drnn_layer = GRU(64, return_sequences=True)(drnn_layer)
#     drnn_layer = LSTM(64, return_sequences=True)(drnn_layer)
#     drnn_layer = GRU(64, return_sequences=True)(drnn_layer)

#     # Attention mechanism
#     attention_layer = Attention()([drnn_layer, drnn_layer])

#     # Concatenate attention output with DRNN output
#     concatenated = Concatenate()([drnn_layer, attention_layer])

#     # TimeDistributed layer to output predictions for each time step
#     output_layer = TimeDistributed(Dense(1, activation='linear'))(concatenated)

#     # Build and compile the model
#     model = Model(inputs=input_layer, outputs=output_layer)
#     model.compile(optimizer='adam', loss='mse')

#     return model

# # Load and preprocess the dataset
# def load_and_preprocess_data(file_path, timesteps):
#     data = pd.read_csv(file_path)
#     irradiance_data = data.iloc[:, 1].values

#     # Normalize the data
#     scaler = MinMaxScaler(feature_range=(0, 1))
#     normalized_data = scaler.fit_transform(irradiance_data.reshape(-1, 1))

#     # Function to create sequences of data for time series prediction
#     def create_sequences(data, time_steps):
#         X, y = [], []
#         for i in range(len(data) - time_steps):
#             X.append(data[i:(i + time_steps), 0])
#             y.append(data[i + time_steps, 0])
#         return np.array(X), np.array(y)

#     # Create sequences
#     X, y = create_sequences(normalized_data, timesteps)

#     # Reshape X to be 3D [samples, timesteps, features]
#     X = X.reshape(X.shape[0], timesteps, 1)

#     # Split the data into training and testing sets
#     split = int(0.8 * len(X))
#     X_train, X_test = X[:split], X[split:]
#     y_train, y_test = y[:split], y[split:]

#     return X_train, X_test, y_train, y_test, scaler

# # Parameters
# file_path = '/mnt/data/Banglore.csv'
# timesteps = 10  # Number of timesteps to look back
# features = 1  # Number of features (solar irradiance)

# # Load and preprocess data
# X_train, X_test, y_train, y_test, scaler = load_and_preprocess_data(file_path, timesteps)

# # Model input shape
# input_shape = (timesteps, features)

# # Create and compile the hybrid model
# model = create_hybrid_model(input_shape)

# # Train the model
# model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# # Evaluate the model
# loss = model.evaluate(X_test, y_test)
# print(f'Test Loss: {loss}')

# # Example predictions
# predictions = model.predict(X_test[:10])
# predictions = scaler.inverse_transform(predictions.reshape(-1, 1))  # Inverse transform to original scale

# print(predictions[:10])  # Display first 10 predictions

# import numpy as np
# import pandas as pd

# # # Define the function to calculate MAPE
# # def calculate_mape(y_true, y_pred):
# #     return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

# # Number of iterations
# num_iterations = 3

# # Initialize matrices to store evaluation metrics across iterations
# rmse_matrix = np.zeros((num_iterations, len(models)))
# mae_matrix = np.zeros((num_iterations, len(models)))
# mse_matrix = np.zeros((num_iterations, len(models)))
# r2_matrix = np.zeros((num_iterations, len(models)))
# mape_matrix = np.zeros((num_iterations, len(models)))

# All_model_dict = []
# y_pred_list = []

# # Initialize dictionary to track minimum metrics and corresponding iterations and models
# min_iteration_metrics = {
#     "RMSE": {"value": float('inf'), "iteration": None, "model": None},
#     "MSE": {"value": float('inf'), "iteration": None, "model": None},
#     "MAE": {"value": float('inf'), "iteration": None, "model": None},
#     "R2": {"value": float('-inf'), "iteration": None, "model": None},
#     "MAPE": {"value": float('inf'), "iteration": None, "model": None}
# }

# # Iteration loop
# for iteration in range(num_iterations):
#     print(f"\nIteration {iteration + 1}/{num_iterations}")

#     # Iterate over each model
#     for model_index, (model_name, model_creator) in enumerate(models):
#         # Train and predict
#         # Check shapes of X_train and y_train and ensure they are compatible
#         print("Shape of X_train:", X_train.shape)
#         print("Shape of y_train:", y_train.shape)
#         y_pred = train_and_predict_model(model_creator, X_train, y_train, X_test, scaler)

#         # Inverse transform the scaled true values
#         y_test_inv = scaler.inverse_transform(y_test)

#         # Ensure that the lengths match by trimming the longer prediction
#         min_length = min(len(y_pred), len(y_test_inv))
#         y_pred = y_pred[:min_length]
#         y_test_inv = y_test_inv[:min_length]

#         # Calculate metrics
#         rmse = calculate_rmse(y_test_inv, y_pred)
#         mse = calculate_mse(y_test_inv, y_pred)
#         mae = calculate_mae(y_test_inv, y_pred)
#         r2 = calculate_r2(y_test_inv, y_pred)
#         mape = calculate_mape(y_test_inv, y_pred)

#         # Store metrics in matrices
#         rmse_matrix[iteration, model_index] = rmse
#         mae_matrix[iteration, model_index] = mae
#         mse_matrix[iteration, model_index] = mse
#         r2_matrix[iteration, model_index] = r2
#         mape_matrix[iteration, model_index] = mape

#         # Update the minimum values and corresponding iteration
#         if rmse < min_iteration_metrics["RMSE"]["value"]:
#             min_iteration_metrics["RMSE"]["value"] = rmse
#             min_iteration_metrics["RMSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["RMSE"]["model"] = model_name
#         if mse < min_iteration_metrics["MSE"]["value"]:
#             min_iteration_metrics["MSE"]["value"] = mse
#             min_iteration_metrics["MSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MSE"]["model"] = model_name
#         if mae < min_iteration_metrics["MAE"]["value"]:
#             min_iteration_metrics["MAE"]["value"] = mae
#             min_iteration_metrics["MAE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAE"]["model"] = model_name
#         if r2 > min_iteration_metrics["R2"]["value"]:
#             min_iteration_metrics["R2"]["value"] = r2
#             min_iteration_metrics["R2"]["iteration"] = iteration + 1
#             min_iteration_metrics["R2"]["model"] = model_name
#         if mape < min_iteration_metrics["MAPE"]["value"]:
#             min_iteration_metrics["MAPE"]["value"] = mape
#             min_iteration_metrics["MAPE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAPE"]["model"] = model_name

#         # Print results for the current iteration and model
#         print(f"\n{model_name} - Iteration {iteration + 1}")
#         print(f"RMSE: {rmse:.2f}, MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}, MAPE: {mape:.2f}")

#         # Save actual and predicted values in an Excel sheet
#         df_results = pd.DataFrame({'Actual': np.squeeze(y_test_inv), 'Predicted': np.squeeze(y_pred)})

# # Number of iterations
# num_iterations = 2

# # Initialize matrices to store evaluation metrics across iterations
# rmse_matrix = np.zeros((num_iterations, len(models)))
# mae_matrix = np.zeros((num_iterations, len(models)))
# mse_matrix = np.zeros((num_iterations, len(models)))
# r2_matrix = np.zeros((num_iterations, len(models)))

# All_model_dict = []
# y_pred_list = []

# # Iteration loop
# for iteration in range(num_iterations):
#     print(f"\nIteration {iteration + 1}/{num_iterations}")

#     # Iterate over each model
#     for model_index, (model_name, model_creator) in enumerate(models):
#         # Train and predict
#         # Check shapes of X_train and y_train and ensure they are compatible
#         print("Shape of X_train:", X_train.shape)
#         print("Shape of y_train:", y_train.shape)
#         y_pred = train_and_predict_model(model_creator, X_train, y_train, X_test, scaler)

#         # Inverse transform the scaled true values
#         y_test_inv = scaler.inverse_transform(y_test)
#         # print("y pred is",y_pred)
#         # print("y pred inv is",y_test_inv)

#         # Ensure that the lengths match by trimming the longer prediction
#         min_length = min(len(y_pred), len(y_test_inv))
#         y_pred = y_pred[:min_length]
#         y_test_inv = y_test_inv[:min_length]

#         # Calculate metrics
#         rmse = calculate_rmse(y_test_inv, y_pred)
#         mse = calculate_mse(y_test_inv, y_pred)
#         mae = calculate_mae(y_test_inv, y_pred)
#         r2 = calculate_r2(y_test_inv, y_pred)


#         # Store metrics in matrices
#         rmse_matrix[iteration, model_index] = rmse
#         mae_matrix[iteration, model_index] = mae
#         mse_matrix[iteration, model_index] = mse
#         r2_matrix[iteration, model_index] = r2

#         # Update the minimum values and corresponding iteration
#         if rmse < min_iteration_metrics["RMSE"]["value"]:
#             min_iteration_metrics["RMSE"]["value"] = rmse
#             min_iteration_metrics["RMSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["RMSE"]["model"] = model_name
#         if mse < min_iteration_metrics["MSE"]["value"]:
#             min_iteration_metrics["MSE"]["value"] = mse
#             min_iteration_metrics["MSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MSE"]["model"] = model_name
#         if mae < min_iteration_metrics["MAE"]["value"]:
#             min_iteration_metrics["MAE"]["value"] = mae
#             min_iteration_metrics["MAE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAE"]["model"] = model_name
#         if r2 > min_iteration_metrics["R2"]["value"]:
#             min_iteration_metrics["R2"]["value"] = r2
#             min_iteration_metrics["R2"]["iteration"] = iteration + 1
#             min_iteration_metrics["R2"]["model"] = model_name

#         # Print results for the current iteration and model
#         print(f"\n{model_name} - Iteration {iteration + 1}")
#         print(f"RMSE: {rmse:.2f}, MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}")


#         # Save actual and predicted values in an Excel sheet
#         df_results = pd.DataFrame({'Actual': np.squeeze(y_test_inv), 'Predicted': np.squeeze(y_pred)})

# # Number of iterations
# num_iterations = 3

# # Initialize matrices to store evaluation metrics across iterations
# rmse_matrix = np.zeros((num_iterations, len(models)))
# mae_matrix = np.zeros((num_iterations, len(models)))
# mse_matrix = np.zeros((num_iterations, len(models)))
# r2_matrix = np.zeros((num_iterations, len(models)))
# mape_matrix  = np.zeros((num_iterations, len(models)))
# All_model_dict = []
# y_pred_list = []

# # Iteration loop
# for iteration in range(num_iterations):
#     print(f"\nIteration {iteration + 1}/{num_iterations}")

#     # Iterate over each model
#     for model_index, (model_name, model_creator) in enumerate(models):
#         # Train and predict
#         y_pred = train_and_predict_model(model_creator, X_train, y_train, X_test, scaler)

#         # Inverse transform the scaled true values
#         y_test_inv = scaler.inverse_transform(y_test)
#         # print("y pred is",y_pred)
#         # print("y pred inv is",y_test_inv)

#         # Ensure that the lengths match by trimming the longer prediction
#         min_length = min(len(y_pred), len(y_test_inv))
#         y_pred = y_pred[:min_length]
#         y_test_inv = y_test_inv[:min_length]

#         # Calculate metrics
#         rmse = calculate_rmse(y_test_inv, y_pred)
#         mse = calculate_mse(y_test_inv, y_pred)
#         mae = calculate_mae(y_test_inv, y_pred)
#         r2 = calculate_r2(y_test_inv, y_pred)
#         mape= calculate_mape(y_test_inv, y_pred)

#         # Store metrics in matrices
#         rmse_matrix[iteration, model_index] = rmse
#         mae_matrix[iteration, model_index] = mae
#         mse_matrix[iteration, model_index] = mse
#         r2_matrix[iteration, model_index] = r2
#         mape_matrix[iteration, model_index] = mape

#         # Update the minimum values and corresponding iteration
#         if rmse < min_iteration_metrics["RMSE"]["value"]:
#             min_iteration_metrics["RMSE"]["value"] = rmse
#             min_iteration_metrics["RMSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["RMSE"]["model"] = model_name
#         if mse < min_iteration_metrics["MSE"]["value"]:
#             min_iteration_metrics["MSE"]["value"] = mse
#             min_iteration_metrics["MSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MSE"]["model"] = model_name
#         if mae < min_iteration_metrics["MAE"]["value"]:
#             min_iteration_metrics["MAE"]["value"] = mae
#             min_iteration_metrics["MAE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAE"]["model"] = model_name
#         if r2 > min_iteration_metrics["R2"]["value"]:
#             min_iteration_metrics["R2"]["value"] = r2
#             min_iteration_metrics["R2"]["iteration"] = iteration + 1
#             min_iteration_metrics["R2"]["model"] = model_name
#         if mape < min_iteration_metrics["MAPE"]["value"]:
#             min_iteration_metrics["MAPE"]["value"] = mape
#             min_iteration_metrics["MAPE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAPE"]["model"] = model_na

#         # Print results for the current iteration and model
#         print(f"\n{model_name} - Iteration {iteration + 1}")
#         print(f"RMSE: {rmse:.2f}, MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f},mape: {mape:.2f}")


#         # Save actual and predicted values in an Excel sheet
#         df_results = pd.DataFrame({'Actual': np.squeeze(y_test_inv), 'Predicted': np.squeeze(y_pred)})
#         df_results.to_excel(f'{model_name}iteration{iteration + 1}.xlsx', index=False)


#         y_pred_list.append(y_pred)
#         model_name = dict()
#         model_name[str(y_pred)] = rmse
#         All_model_dict.append(model_name)
#     # print(All_model_dict)
#     # print(y_pred_list)


# # Print the minimum values and corresponding iteration and model
# print("\nMinimum Values Across Iterations:")
# for metric, value_info in min_iteration_metrics.items():
#   print(f"{metric} - Model: {value_info['model']}, Value: {value_info['value']:.2f} (Iteration: {value_info['iteration']})")



# # Print matrices for all iterations
# print("\nRMSE Matrix:")
# print(rmse_matrix)
# print("\nMAE Matrix:")
# print(mae_matrix)
# print("\nMSE Matrix:")
# print(mse_matrix)
# print("\nR2 Matrix:")
# print(r2_matrix)
# print("\nMAPE Matrix:")
# print(mape_matrix)

# # Calculate average metrics across iterations
# avg_rmse = np.mean(rmse_matrix, axis=0)
# avg_mae = np.mean(mae_matrix, axis=0)
# avg_mse = np.mean(mse_matrix, axis=0)
# avg_r2 = np.mean(r2_matrix, axis=0)
# avg_mape = np.mean(mape_matrix, axis=0)

# # Print average metrics
# print("\nAverage Metrics Across Models:")
# for i, (model_name, _) in enumerate(models):
#     print(f"{model_name}: RMSE={avg_rmse[i]:.2f}, MAE={avg_mae[i]:.2f}, MSE={avg_mse[i]:.2f}, R2={avg_r2[i]:.2f},mape={avg_mape[i]}:.2f}")

# # Calculate and print minimum values and max for R2 across all iterations
# min_r2 = np.min(avg_r2)
# min_rmse = np.min(rmse_matrix)
# min_mae = np.min(mae_matrix)
# min_mse = np.min(mse_matrix)
# max_r2 = np.max(r2_matrix)
# min_mape = np.min(mape_matrix)

# print("\nOverall Minimum Values:")
# print(f"RMSE: {min_rmse:.2f}")
# print(f"MAE: {min_mae:.2f}")
# print(f"MSE: {min_mse:.2f}")
# print(f"R2: {max_r2:.2f}")
# print(f"MAPE: {min_mape:.2f}")



# # To take the min and max of all the iteration of scores -> Model-wise
# def row_major_to_column_major(score_matrix):
#   row_major_array = np.array(score_matrix)
#   # Transpose the array to convert it to column-major order
#   column_major_array = row_major_array.T
#   return column_major_array

# score_list = ["rmse_matrix","mae_matrix","mse_matrix","r2_matrix","mape_matrix"]

# score_dict = {"rmse_matrix": rmse_matrix,
#                "mae_matrix": mae_matrix,
#                "mse_matrix": mse_matrix,
#                "r2_matrix": r2_matrix,
#                  "mape_matrix": mape_matrix

# for i in score_list:
#   score_dict[i] = row_major_to_column_major(score_dict[i])
# }

# # Number of iterations
# num_iterations = 3

# # Iteration loop
# for iteration in range(num_iterations):
#     print(f"\nIteration {iteration + 1}/{num_iterations}")

#     # Iterate over each model
#     for model_name, model_creator in models:
#         # Train and predict
#         y_pred = train_and_predict_model(model_creator, X_train, y_train, X_test, scaler)

#         # Inverse transform the scaled true values
#         y_test_inv = scaler.inverse_transform(y_test)

#         # Ensure that the lengths match by trimming the longer prediction
#         min_length = min(len(y_pred), len(y_test_inv))
#         y_pred = y_pred[:min_length]
#         y_test_inv = y_test_inv[:min_length]

#         # Calculate metrics
#         rmse = calculate_rmse(y_test_inv, y_pred)
#         mse = calculate_mse(y_test_inv, y_pred)
#         mae = calculate_mae(y_test_inv, y_pred)
#         mape = calculate_mape(y_test_inv, y_pred)
#         r2 = calculate_r2(y_test_inv, y_pred)


#         # Store metrics in matrices
#         rmse_matrix[iteration, model_index] = rmse
#         mae_matrix[iteration, model_index] = mae
#         mse_matrix[iteration, model_index] = mse
#         r2_matrix[iteration, model_index] = r2

#         # Update the minimum values and corresponding iteration
#         if rmse < min_iteration_metrics["RMSE"]["value"]:
#             min_iteration_metrics["RMSE"]["value"] = rmse
#             min_iteration_metrics["RMSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["RMSE"]["model"] = model_name
#         if mse < min_iteration_metrics["MSE"]["value"]:
#             min_iteration_metrics["MSE"]["value"] = mse
#             min_iteration_metrics["MSE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MSE"]["model"] = model_name
#         if mae < min_iteration_metrics["MAE"]["value"]:
#             min_iteration_metrics["MAE"]["value"] = mae
#             min_iteration_metrics["MAE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAE"]["model"] = model_name
#         if mape < min_iteration_metrics["MAPE"]["value"]:
#             min_iteration_metrics["MAPE"]["value"] = mape
#             min_iteration_metrics["MAPE"]["iteration"] = iteration + 1
#             min_iteration_metrics["MAPE"]["model"] = model_name
#         if r2 > min_iteration_metrics["R2"]["value"]:
#             min_iteration_metrics["R2"]["value"] = r2
#             min_iteration_metrics["R2"]["iteration"] = iteration + 1
#             min_iteration_metrics["R2"]["model"] = model_name

#         # Update the minimum values across all iterations
#         min_metrics["RMSE"] = min(min_metrics["RMSE"], rmse)
#         min_metrics["MSE"] = min(min_metrics["MSE"], mse)
#         min_metrics["MAE"] = min(min_metrics["MAE"], mae)
#         min_metrics["MAPE"] = min(min_metrics["MAPE"], mape)
#         min_metrics["R2"] = max(min_metrics["R2"], r2)

#         # Store results in the dictionary
#         results[model_name]["RMSE"].append(rmse)
#         results[model_name]["MAE"].append(mae)
#         results[model_name]["MSE"].append(mse)
#         results[model_name]["MAPE"].append(mape)
#         results[model_name]["R2"].append(r2)

#         # Save actual and predicted values in an Excel sheet
#         df_results = pd.DataFrame({'Actual': np.squeeze(y_test_inv), 'Predicted': np.squeeze(y_pred)})
#         df_results.to_excel(f'{model_name}iteration{iteration + 1}.xlsx', index=False)

#         # Print results for the current iteration and model
#         print(f"\n{model_name} - Iteration {iteration + 1}")
#         print(f"RMSE: {rmse:.2f}, MSE: {mse:.2f}, MAE: {mae:.2f}, MAPE: {mape:.2f}, R2: {r2:.2f}")

#         # Calculate average metrics across iterations
#         avg_rmse = np.mean(rmse_matrix, axis=0)
#         avg_mae = np.mean(mae_matrix, axis=0)
#         avg_mse = np.mean(mse_matrix, axis=0)
#         avg_r2 = np.mean(r2_matrix, axis=0)

# # Print average metrics
# print("\nAverage Metrics Across Models:")
# for i, (model_name, _) in enumerate(models):
#   print(f"{model_name}: RMSE={avg_rmse[i]:.2f}, MAE={avg_mae[i]:.2f}, MSE={avg_mse[i]:.2f}, R2={avg_r2[i]:.2f}")

# # Print the minimum values and corresponding iteration and model
# print("\nMinimum Values Across Iterations:")
# for metric, value_info in min_iteration_metrics.items():
#     print(f"{metric} - Model: {value_info['model']}, Value: {value_info['value']:.2f} (Iteration: {value_info['iteration']})")

# print("\nOverall Minimum Values:")
# for metric, value in min_metrics.items():
#     print(f"{metric}: {value:.2f}")



# Taking key value pair as y_pred:rmse of each model
of_LSTM = All_model_dict[0::5]
of_BiLSTM = All_model_dict[1::5]
of_GRU = All_model_dict[2::5]
of_CNN = All_model_dict[3::5]
of_RNN = All_model_dict[4::5]

RMSE_of_LSTM = []
RMSE_of_BiLSTM = []
RMSE_of_GRU = []
RMSE_of_CNN = []
RMSE_of_RNN = []
for i in range(num_iterations):
  # To store the RMSE of LSTM in each iteration
  RMSE_of_LSTM.append(list(of_LSTM[i].values()))

  # To store the RMSE of BiLSTM in each iteration
  RMSE_of_BiLSTM.append(list(of_BiLSTM[i].values()))

  # To store the RMSE of GRU in each iteration
  RMSE_of_GRU.append(list(of_GRU[i].values()))

  # To store the RMSE of CNN in each iteration
  RMSE_of_CNN.append(list(of_CNN[i].values()))

  # To store the RMSE of RNN in each iteration
  RMSE_of_RNN.append(list(of_RNN[i].values()))

# To find the index of mininmum of RMSE of all the models
index_of_min_RMSE_of_LSTM = RMSE_of_LSTM.index(min(RMSE_of_LSTM))
index_of_min_RMSE_of_BiLSTM = RMSE_of_BiLSTM.index(min(RMSE_of_BiLSTM))
index_of_min_RMSE_of_GRU = RMSE_of_GRU.index(min(RMSE_of_GRU))
index_of_min_RMSE_of_CNN = RMSE_of_CNN.index(min(RMSE_of_CNN))
index_of_min_RMSE_of_RNN = RMSE_of_RNN.index(min(RMSE_of_RNN))

# print(index_of_min_RMSE_of_RNN)
# print(RMSE_of_LSTM)
# print(RMSE_of_BiLSTM)
# print(RMSE_of_GRU)
# print(RMSE_of_CNN)
# print(RMSE_of_RNN)

# y_pred corresponding to lowest RMSE of each model
# For LSTM
y_pred_corresponding_to_lowest_RMSE_of_LSTM = y_pred_list[(index_of_min_RMSE_of_LSTM*len(models))+0]

# For BiLSTM
y_pred_corresponding_to_lowest_RMSE_of_BiLSTM = y_pred_list[(index_of_min_RMSE_of_BiLSTM*len(models))+1]

# For GRU
y_pred_corresponding_to_lowest_RMSE_of_GRU = y_pred_list[(index_of_min_RMSE_of_GRU*len(models))+2]

# For CNN
y_pred_corresponding_to_lowest_RMSE_of_CNN = y_pred_list[(index_of_min_RMSE_of_CNN*len(models))+3]

# For RNN
y_pred_corresponding_to_lowest_RMSE_of_RNN = y_pred_list[(index_of_min_RMSE_of_RNN*len(models))+4]

# print(y_pred_corresponding_to_lowest_RMSE_of_GRU)
# print(y_pred_corresponding_to_lowest_RMSE_of_RNN)
y_test=scaler.inverse_transform(y_test.reshape(-1,1))



# # Scatter Plot of LSTM corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_LSTM

# # Create scatter plot
# plt.scatter(x, y)

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Scatter Plot of LSTM')

# # Display the plot
# plt.show()

# # Line Plot of LSTM corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_LSTM

# # Create line plot for x values (Y-Test)
# plt.plot(x, label='Y-Test')  # Line for x

# # Create line plot for y values (Y-Predicted)
# plt.plot(y, label='Y-Predicted')  # Line for y

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Line Plot of LSTM')

# # Display the plot
# plt.legend()
# plt.show()



# # Line Plot of LSTM corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_LSTM

# # Create line plot for x values (Y-Test)
# plt.plot(x, label='Y-Test')  # Line for x

# # Create line plot for y values (Y-Predicted)
# plt.plot(y, label='Y-Predicted')  # Line for y

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Line Plot of LSTM')

# # Display the plot
# plt.legend()
# plt.show()

# # Line Plot of BiLSTM corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_BiLSTM

# # Create line plot for x values (Y-Test)
# plt.plot(x, label='Y-Test')  # Line for x

# # Create line plot for y values (Y-Predicted)
# plt.plot(y, label='Y-Predicted')  # Line for y

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Line Plot of BiLSTM')

# # Display the plot
# plt.legend()
# plt.show()

# # Scatter Plot of BiLSTM corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_BiLSTM

# # Create scatter plot
# plt.scatter(x, y)

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Scatter Plot on BiLSTM')

# # Display the plot
# plt.show()

# # Scatter Plot of GRU corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_GRU

# # Create scatter plot
# plt.scatter(x, y)

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Scatter Plotof GRU')

# # Display the plot
# plt.show()

# # Line Plot of GRU corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_GRU

# # Create line plot for x values (Y-Test)
# plt.plot(x, label='Y-Test')  # Line for x

# # Create line plot for y values (Y-Predicted)
# plt.plot(y, label='Y-Predicted')  # Line for y

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Line Plot of GRU')

# # Display the plot
# plt.legend()
# plt.show()

# # Scatter Plot of CNN corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_CNN

# # Create scatter plot
# plt.scatter(x, y)

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Scatter Plot of CNN')

# # Display the plot
# plt.show()

# # Line Plot of CNN corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_CNN

# # Create line plot for x values (Y-Test)
# plt.plot(x, label='Y-Test')  # Line for x

# # Create line plot for y values (Y-Predicted)
# plt.plot(y, label='Y-Predicted')  # Line for y

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Line Plot of CNN')

# # Display the plot
# plt.legend()
# plt.show()

# # Scatter Plot of RNN corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_RNN

# # Create scatter plot
# plt.scatter(x, y)

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Scatter Plot of RNN')

# # Display the plot
# plt.show()

# # Line Plot of RNN corresponding to lowest RMSE score
# x = y_test
# y = y_pred_corresponding_to_lowest_RMSE_of_RNN

# # Create line plot for x values (Y-Test)
# plt.plot(x, label='Y-Test')  # Line for x

# # Create line plot for y values (Y-Predicted)
# plt.plot(y, label='Y-Predicted')  # Line for y

# # Add labels and title
# plt.xlabel('Y-Test')
# plt.ylabel('Y-Predicted')
# plt.title('Line Plot of RNN')

# # Display the plot
# plt.legend()
# plt.show()

# # Define the data for the box plot
# data = [rmse_matrix.T, mae_matrix.T, mse_matrix.T, r2_matrix.T]

# # Define the labels for the x-axis (models)
# labels = ["LSTM", "BiLSTM", "GRU", "CNN", "RNN"]

# # Define the measurements for the y-axis
# measurements = ["RMSE", "MAE", "MSE", "R^2"]

# # Create subplots for each measurement
# fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))

# # Flatten the axes array for easier iteration
# axes = axes.flatten()

# # Iterate over each measurement and corresponding data
# for ax, measurement, measurement_data in zip(axes, measurements, data):
#     # Create the box plot for the current measurement
#     ax.boxplot(measurement_data.T, labels=labels, patch_artist=True)
#     ax.set_title(f"{measurement} Distribution Across Models", fontsize=14)
#     ax.set_ylabel(measurement, fontsize=12)
#     ax.tick_params(axis='x', labelsize=12)
#     ax.tick_params(axis='y', labelsize=12)
#     ax.grid(axis='y', linestyle='--', alpha=0.7)

# # Hide unused subplots
# for i in range(len(measurements), len(axes)):
#     fig.delaxes(axes[i])

# # Add x-axis label to the bottom plot
# fig.text(0.5, 0.04, 'Models', ha='center', fontsize=12)

# plt.tight_layout(pad=3.0)
# plt.show()

# from google.colab import drive
# drive.mount('/content/drive')

# df.describe()

# import matplotlib.pyplot as plt
# from statsmodels.graphics.tsaplots import plot_acf
# plot_acf(df['TS'], lags=50)  # Adjust the number of lags as needed
# plt.xlabel('Lag')
# plt.ylabel('Autocorrelation')
# plt.title('Autocorrelation Function (ACF) for "VALUE" Column')
# plt.show()

# import matplotlib.pyplot as plt
# from statsmodels.graphics.tsaplots import plot_pacf

# plot_pacf(df['TS'], lags=50)  # Adjust the number of lags as needed
# plt.xlabel('Lag')
# plt.ylabel('Partial Autocorrelation')
# plt.title('Partial Autocorrelation Function (PACF) for "VALUE" Column')
# plt.show()

# !pip install pandas statsmodels matplotlib

# import pandas as pd
# import statsmodels.tsa.seasonal as seasonal_decompose
# import matplotlib.pyplot as plt

# df.info()



help(seasonal_decompose)

# import pandas as pd
# from statsmodels.tsa.seasonal import seasonal_decompose
# import matplotlib.pyplot as plt

# # Assuming df is your time series DataFrame
# subset_df = df[:60]  # Selecting the first 60 data points as the subset

# # Extracting the time series column from the DataFrame
# ts = subset_df['TS']

# # Perform seasonal decomposition on the subset
# result = seasonal_decompose(ts, model='additive', period=5)

# # Plot the original data, the trend, the seasonality, and the residuals
# fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(10, 8))
# result.observed.plot(ax=axes[0], title='Observed')
# result.trend.plot(ax=axes[1], title='Trend')
# result.seasonal.plot(ax=axes[2], title='Seasonal')
# result.resid.plot(ax=axes[3], title='Residuals (Spikes)')
# plt.tight_layout()
# plt.show()

# plt.figure(figsize=(12, 6))
# plt.plot(y_test_inv, label='Actual', marker='o')
# plt.plot(y_pred, label='Predicted', marker='x')
# plt.legend()
# plt.title('Actual vs. Predicted Values')
# plt.xlabel('Data Point')
# plt.ylabel('Value')
# plt.grid(True)
# plt.show()

# # Assuming y_test contains the actual values and y_pred contains the predicted values
# plt.figure(figsize=(8, 8))
# plt.scatter(y_test_inv,y_pred, s=30, c='blue', alpha=0.5)
# plt.plot([y_test_inv.min(), y_test_inv.max()], [y_test_inv.min(), y_test_inv.max()], 'k--', lw=2)
# plt.title('Scatter Plot of Actual vs. Predicted Values')
# plt.xlabel('Actual Values')
# plt.ylabel('Predicted Values')
# plt.grid(True)
# plt.show()

# import matplotlib.pyplot as plt

# def plot_boxplot(y_test_inv, y_pred):
#     plt.figure(figsize=(8, 8))
#     plt.boxplot([y_test_inv, y_pred], patch_artist=True, labels=['Actual', 'Predicted'])
#     plt.title('Box Plot of Actual vs. Predicted Values')
#     plt.xlabel('Data Type')
#     plt.ylabel('Values')
#     plt.grid(True)
#     plt.show()

# # Example usage:
# # Assuming y_test and y_pred are arrays of actual and predicted values respectively
# # Ensure both y_test and y_pred have the same length and contain numerical data
# import numpy as np
# y_test_inv = np.random.rand(100)  # Example y_test data
# y_pred = np.random.rand(100)  # Example y_pred data
# plot_boxplot(y_test_inv, y_pred)



import matplotlib.pyplot as plt

# Average metrics for models
average_metrics = {
    "LSTM": {"RMSE": 33.47, "MSE": 1120.60, "MAE": 18.64, "R2": 0.99, "MAPE": 9.84},
    "BiLSTM": {"RMSE": 34.27, "MSE": 1176.33, "MAE": 19.51, "R2": 0.99, "MAPE": 9.59},
    "GRU": {"RMSE": 33.71, "MSE": 1136.97, "MAE": 19.12, "R2": 0.99, "MAPE": 8.88},
    "CNN": {"RMSE": 350.17, "MSE": 122664.92, "MAE": 264.37, "R2": -0.55, "MAPE": 119.50},
    "RNN": {"RMSE": 32.72, "MSE": 1071.23, "MAE": 17.71, "R2": 0.99, "MAPE": 9.16},
    "Hybrid": {"RMSE": 33.73, "MSE": 1140.77, "MAE": 18.05, "R2": 0.99, "MAPE": 9.14}
}

# Extract metrics
models = list(average_metrics.keys())
rmse = [average_metrics[model]["RMSE"] for model in models]
mse = [average_metrics[model]["MSE"] for model in models]
mae = [average_metrics[model]["MAE"] for model in models]
r2 = [average_metrics[model]["R2"] for model in models]
mape = [average_metrics[model]["MAPE"] for model in models]

# Create scatter plots
fig, ax = plt.subplots(figsize=(12, 8))

ax.scatter(models, rmse, label='RMSE', color='r', marker='o')
ax.scatter(models, mse, label='MSE', color='g', marker='s')
ax.scatter(models, mae, label='MAE', color='b', marker='^')
ax.scatter(models, r2, label='R2', color='m', marker='x')
ax.scatter(models, mape, label='MAPE', color='c', marker='d')

ax.set_xlabel('Models')
ax.set_ylabel('Metrics')
ax.set_title('Average Metrics for Different Models')
ax.legend()
ax.grid(True)

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Average metrics for models
average_metrics = {
    "LSTM": {"RMSE": 33.47, "MSE": 1120.60, "MAE": 18.64, "R2": 0.99, "MAPE": 9.84},
    "BiLSTM": {"RMSE": 34.27, "MSE": 1176.33, "MAE": 19.51, "R2": 0.99, "MAPE": 9.59},
    "GRU": {"RMSE": 33.71, "MSE": 1136.97, "MAE": 19.12, "R2": 0.99, "MAPE": 8.88},
    "CNN": {"RMSE": 350.17, "MSE": 122664.92, "MAE": 264.37, "R2": -0.55, "MAPE": 119.50},
    "RNN": {"RMSE": 32.72, "MSE": 1071.23, "MAE": 17.71, "R2": 0.99, "MAPE": 9.16},
    "Hybrid": {"RMSE": 33.73, "MSE": 1140.77, "MAE": 18.05, "R2": 0.99, "MAPE": 9.14}
}

# Extract metrics
models = list(average_metrics.keys())
rmse = [average_metrics[model]["RMSE"] for model in models]
mse = [average_metrics[model]["MSE"] for model in models]
mae = [average_metrics[model]["MAE"] for model in models]
r2 = [average_metrics[model]["R2"] for model in models]
mape = [average_metrics[model]["MAPE"] for model in models]

# Create scatter plots
fig, ax = plt.subplots(figsize=(12, 8))

ax.scatter(models, rmse, label='RMSE', color='r', marker='o')
ax.scatter(models, mse, label='MSE', color='g', marker='s')
ax.scatter(models, mae, label='MAE', color='b', marker='^')
ax.scatter(models, r2, label='R2', color='m', marker='x')
ax.scatter(models, mape, label='MAPE', color='c', marker='d')

ax.set_xlabel('Models')
ax.set_ylabel('Metrics')
ax.set_title('Average Metrics for Different Models')
ax.legend()
ax.grid(True)

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Create synthetic data for illustration
# Assuming actual values are normally distributed
np.random.seed(0)
actual_values = np.random.normal(loc=0, scale=1, size=2000)

# Assuming synthetic predictions based on average metrics for illustration
def generate_synthetic_predictions(actual_values, rmse):
    noise = np.random.normal(loc=0, scale=rmse, size=actual_values.shape)
    return actual_values + noise

# Average metrics for traditional models
traditional_metrics = {
    "LSTM": {"RMSE": 33.47},
    "BiLSTM": {"RMSE": 34.27},
    "GRU": {"RMSE": 33.71},
    "CNN": {"RMSE": 350.17},
    "RNN": {"RMSE": 32.72}
}

# Average metrics for hybrid models
hybrid_metrics = {
    "Hybrid_LSTM_RNN": {"RMSE": 33.73},
   # Assuming same RMSE for illustration
}

# Generate synthetic predictions
best_test_predictions = {}
for model_name, metrics in traditional_metrics.items():
    best_test_predictions[model_name] = generate_synthetic_predictions(actual_values, metrics["RMSE"])

for model_name, metrics in hybrid_metrics.items():
    best_test_predictions[model_name] = generate_synthetic_predictions(actual_values, metrics["RMSE"])

# Define traditional and hybrid models
traditional_models = list(traditional_metrics.keys())
hybrid_models = list(hybrid_metrics.keys())

# Create subplots for each comparison: traditional vs hybrid
fig, axes = plt.subplots(nrows=len(traditional_models), ncols=len(hybrid_models), figsize=(20, 25))
axes = axes.flatten()

for i, traditional_model in enumerate(traditional_models):
    for j, hybrid_model in enumerate(hybrid_models):
        idx = i * len(hybrid_models) + j
        sns.scatterplot(x=actual_values, y=best_test_predictions[traditional_model], ax=axes[idx], label=f'{traditional_model} Predictions', color='blue')
        sns.scatterplot(x=actual_values, y=best_test_predictions[hybrid_model], ax=axes[idx], label=f'{hybrid_model} Predictions', color='green')
        sns.lineplot(x=actual_values, y=actual_values, ax=axes[idx], color='red', label='Actual')
        axes[idx].set_title(f'{traditional_model} vs {hybrid_model}')
        axes[idx].set_xlabel('Actual Values')
        axes[idx].set_ylabel('Predicted Values')
        axes[idx].legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Create synthetic data for illustration
np.random.seed(0)
actual_values = np.random.normal(loc=0, scale=1, size=2000)

# Function to generate synthetic predictions based on RMSE
def generate_synthetic_predictions(actual_values, rmse):
    noise = np.random.normal(loc=0, scale=rmse, size=actual_values.shape)
    return actual_values + noise

# Average metrics for traditional models
traditional_metrics = {
    "LSTM": {"RMSE": 31.56},
    "BiLSTM": {"RMSE": 31.89},
    "GRU": {"RMSE": 32.00},
    "CNN": {"RMSE": 346.11},
    "RNN": {"RMSE": 32.06}
}

# Average metrics for hybrid models
hybrid_metrics = {
    "Hybrid_LSTM_RNN": {"RMSE": 32.02},
    # "Hybrid_RNN_GRU": {"RMSE": 32.02}  # Assuming same RMSE for illustration
}

# Generate synthetic predictions
best_test_predictions = {}
for model_name, metrics in traditional_metrics.items():
    best_test_predictions[model_name] = generate_synthetic_predictions(actual_values, metrics["RMSE"])

for model_name, metrics in hybrid_metrics.items():
    best_test_predictions[model_name] = generate_synthetic_predictions(actual_values, metrics["RMSE"])

# Define traditional and hybrid models
traditional_models = list(traditional_metrics.keys())
hybrid_models = list(hybrid_metrics.keys())

# Create subplots for each comparison: traditional vs hybrid
fig, axes = plt.subplots(nrows=len(traditional_models), ncols=len(hybrid_models), figsize=(20, 25))
axes = axes.flatten()

for i, traditional_model in enumerate(traditional_models):
    for j, hybrid_model in enumerate(hybrid_models):
        idx = i * len(hybrid_models) + j
        sns.scatterplot(x=actual_values, y=best_test_predictions[traditional_model], ax=axes[idx], label=f'{traditional_model} Predictions', color='blue')
        sns.scatterplot(x=actual_values, y=best_test_predictions[hybrid_model], ax=axes[idx], label=f'{hybrid_model} Predictions', color='green')
        sns.lineplot(x=actual_values, y=actual_values, ax=axes[idx], color='red', label='Actual')
        axes[idx].set_title(f'{traditional_model} vs {hybrid_model}')
        axes[idx].set_xlabel('Actual Values')
        axes[idx].set_ylabel('Predicted Values')
        axes[idx].legend()

plt.tight_layout()
plt.show()